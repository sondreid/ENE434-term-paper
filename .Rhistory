us_employment
us_retail_employment %>%
model(
classical_decomposition(Employed, type = "additive")
) %>%
components() %>%
autoplot() +
labs(title = "Classical additive decomposition of total
US retail employment")
df <- us_employment
df %>%
model(
classical_decomposition(Employed, type = "additive")
) %>%
components() %>%
autoplot() +
labs(title = "Classical additive decomposition of total
US retail employment")
df %>% classical_decomp <- model(classical_decomposition(Employed, type = "additive"))
df <- us_employment
classical_decomp <- df %>%  model(classical_decomposition(Employed, type = "additive"))
View(classical_decomp)
classical_decomp <- df %>%  model(classical_decomposition(Employed, type = "additive")) %>% components()
View(classical_decomp)
classical_decomp %>%
autoplot() +
labs(title = "Classical additive decomposition of total
US retail employment")
library(fpp3)
attach(us_employment)
df <- us_employment
classical_decomp <- df %>%  model(classical_decomposition(Employed, type = "additive")) %>% components()
classical_decomp %>%
autoplot() +
labs(title = "Classical additive decomposition of total
US retail employment")
ts(classical_decomp)
ts(classical_decomp) %>%
autoplot() +
labs(title = "Classical additive decomposition of total
US retail employment")
library(ggfortify)
ts(classical_decomp) %>%
autoplot() +
labs(title = "Classical additive decomposition of total
US retail employment")
classical_decomp %>%
select(Employed, trend, seasonal) %>%
ts() %>%
autoplot() +
labs(title = "Classical additive decomposition of total
US retail employment")
df <- us_employment
df
View(df)
library(lubridate)
year(df$Month[1])
df <- us_employment %>% mutate(Year = year(Month))
df
View(df)
library(fpp3)
attach(us_employment)
library(ggfortify)
library(lubridate)
df <- us_employment %>% mutate(Year = year(Month))
classical_decomp <- df %>%
filter(Year >= 1990) %>%
model(classical_decomposition(Employed, type = "additive")) %>%
components()
classical_decomp %>%
select(Employed, trend, seasonal) %>%
ts() %>%
autoplot() +
labs(title = "Classical additive decomposition of total
US retail employment")
classical_decomp %>% select(Employed)
library(fpp3)
attach(us_employment)
library(ggfortify)
library(lubridate)
df <- us_employment %>% mutate(Year = year(Month))
classical_decomp <- df %>%
filter(Year >= 1990) %>%
select(Employed, trend, seasonal) %>%
model(classical_decomposition(Employed, type = "additive")) %>%
components()
classical_decomp %>%
ts() %>%
autoplot() +
labs(title = "Classical additive decomposition of total
US retail employment")
View(classical_decomp)
classical_decomp %>% select(Employed)
classical_decomp %<>% select(-Series_ID)
library(magrittr)
classical_decomp %<>% select(-Series_ID)
classical_decomp %<>% select(-.model, -Month, .Series_ID)
classical_decomp %<>% select(-.model, -Month)
classical_decomp
classical_decomp
df <- us_employment %>% mutate(Year = year(Month))
classical_decomp <- df %>%
filter(Year >= 1990) %>%
model(classical_decomposition(Employed, type = "additive")) %>%
components()
classical_decomp
plot(classical_decomp)
install.packages("X11")
library(X11)
install.packages("X11")
install.packages("Rtools")
source("data.r")
source("data.r")
2+2
# Generating y_t
generate_y <- function(fit, n) {
#' Function that passes the standard deviation of the residuals of our optim-
#' al ARIMA model
#' Automatically finds and passes ar and ma terms to the arima.sim (stats pa-
#' ckage), and returns the generated series
sigma <- sd(residuals(fit)$.resid)
ar_terms <- (fit %>%
coefficients %>%
filter(str_detect(term,"ar")))$estimate %>%
c(.) # AR terms and their coefficients
sma_terms <- (fit %>% coefficients
%>% filter(str_detect(term, "sma")))$estimate %>%
c(.)
arima_sim_model <- list(order = c(5, 1, 0),
ar = ar_terms,
sma = sma_terms)
y <- arima.sim(n = n,
arima_sim_model,
sd = sigma)
return(y)
}
# Simulation function
simulate <- function(fit, R, train_length , h ) {
#' Function that generates a new series x based on an arima simulation retu-
#' rned by generate_y.
#' Compares two models, and populates which contains a series of forecast e-
30
#' valuation metrics.
#' Returns the populated matrix.
res <- matrix(0,2,5)
colnames(res) <- c("RMSE", "MASE", "MAE", "MAPE", "RMSSE")
rownames(res) <- c("VAR multivariate", "ARIMA yt")
for(i in 1:R){
y <- diff(generate_y(fit, train_length+h))
y_e <- y[1:train_length]
y_t <- y[(train_length+1):(train_length+h)]
x <- c()
x[1] <- y[1]
for (j in 2:(train_length+h)) {
x[j] <- 0.5*y[j-1] + 0.5*x[j-1] + rnorm(n = 1,
mean = 0,
sd = sd(y))
}
x_e <- x[1:train_length]
x_t <- x[(train_length+1):(train_length+h)]
data_x_y = data.frame(date = (1:train_length),
x_e = x_e,
y_e = y_e) %>%
as_tsibble(index = date)
var_multi <- vars:: VAR(data_x_y[,2:3],
p = 1,
type = "const") # VAR(1) model
arima_uni <- data_x_y %>%
model(Arima = ARIMA(y_e ~ 0 + pdq(1,0,0) + PDQ(0,0,0))) # ARIMA pdq(1,0,0) model
var_resids <- y_t - predict(var_multi, n.ahead = h)$fcst$y_e[,1]
arima_resids <- y_t - (arima_uni %>% forecast(h = h))$.mean
res[1,1] <- res[1,1] + RMSE(var_resids)/R
res[2,1] <- res[2,1] + RMSE(arima_resids)/R
res[1,2] <- res[1,2] + MASE(.resid = var_resids,
.train = y_e,
.period = 12)/R
res[2,2] <- res[2,2] + MASE(.resid = arima_resids,
.train = y_e,
.period = 12)/R
res[1,3] <- res[1,3] + MAE(.resid = var_resids)/R
31
res[2,3] <- res[2,3] + MAE(.resid = arima_resids)/R
res[1,4] <- res[1,4] + fabletools::MAPE(.resid = var_resids,
.actual = y_t,
.period = 12)/R
res[2,4] <- res[2,4] + fabletools::MAPE(.resid = arima_resids,
.actual = y_t,
.period = 12)/R
res[1,5] <- res[1,5] + RMSSE(.resid = var_resids,
.train = y_e,
.period = 12)/R
res[2,5] <- res[2,5] + RMSSE(.resid = arima_resids,
.train = y_e,
.period = 12)/R
}
return(res)
}
# Wrapper function
wrapperSim <- function(R, sample_size, test_ratio) {
#' Wrapper function that splits the unemployment series into
#' test and training lengths based on an input sample length and
#' test ratio of the overall series length.
#' Passes this as parameters to the simulate function
cl <- parallel::makeCluster(parallel::detectCores()) ### Make clusters
doParallel::registerDoParallel(cl)
train_length <- floor(sample_size * (1 - test_ratio))
h <- ceiling(sample_size * test_ratio)
print(paste("Training length: ", train_length))
print(paste("h : ", h))
start <- (nrow(unemployment_train_ts) - sample_size)
sim_res <- simulate(fit_arima_optimal, R, train_length, h) %>%
as.data.frame() %>%
mutate("Sample length" = sample_size)
parallel::stopCluster(cl)
return(sim_res) }
summary(fi_garch)
##### Load libraries ######
library(tidyverse)
library(fpp3)
library(kableExtra)
library(fGarch)
library(lubridate)
library(readxl)
library(magrittr)
library(forecast)
library(dynlm)
library(fable)
library(tseries)
## Seasonal decompositon of Danish electricity consumption
cons_comp_dk = cons_ts %>% model(
STL(DK ~ trend(window=7) + season(window="periodic"))
) %>% components
forecast::ggtsdisplay(cons$DK, plot_type='partial',
lag.max = 24,
theme = theme_bw(),
main = "Elecitricity consumption in Denmark ACF and PACF plots ")
# Perform differencing
cons_diff_dk <- cons %>% mutate(DK = difference(DK,7)) %>% dplyr::filter(!is.na(DK)) #Take first order difference
fit_arima_optimal_cons_dk <-
cons %>%
as_tsibble(index = date) %>%
model(arima_optimal =  ARIMA(DK, stepwise = FALSE, approximation = FALSE))
### Data retrieval ##
ets = read_csv("http://jmaurit.github.io/analytics/labs/data/eua-price.csv") ## Carbon pricing data
colnames(ets) = c("date", "price")                                                # Change rownames
elspot = read_csv("http://jmaurit.github.io/norwayeconomy/data_series/elspot.csv") ##
ets["month"] = month(ets$date)
ets["year"] = year(ets$date)
ets_mon = ets %>% group_by(month, year) %>% summarise(
price = mean(price))
ets_mon["day"] = 1
ets_mon = ets_mon %>% arrange(year, month)
ets_mon = ets_mon %>% mutate(date = make_date(year, month, day))
## Convert to monthly data
ets %<>%
mutate(month = month(date)) %>%
group_by(month) %>%
summarise(ets_mon = mean(price),
year = year(date),
month = month,
day  = 1) %>%
ungroup() %>%
mutate(date = make_date(year, month, day ))
# Join carbon pricing data and
power_df <- elspot %>% inner_join(ets_mon[c("price", "date")], by="date")
power_DK_df <- power_df %>% dplyr:: select(DK1, DK2, date, price)
## Scale to mwh
power_DK_df %<>% mutate(DK1 = DK1/1000,
DK2 = DK2/1000)
## Daily consumption data
cons = read_csv2("http://jmaurit.github.io/analytics/labs/data/consumption-per-country_2019_daily.csv")
cons["date"] = as.Date(cons$date, format="%d/%m/%Y")
cons_ts <- tsibble(cons, index=date)
## Seasonal decompositon of Danish electricity consumption
cons_comp_dk = cons_ts %>% model(
STL(DK ~ trend(window=7) + season(window="periodic"))
) %>% components
cons_comp_no = cons_ts %>% model(
STL(NO ~ trend(window=7) + season(window="periodic"))
) %>% components
cons_comp_dk %>%  autoplot()
cons_comp_no %>%  autoplot()
forecast::ggtsdisplay(cons$DK, plot_type='partial',
lag.max = 24,
theme = theme_bw(),
main = "Elecitricity consumption in Denmark ACF and PACF plots ")
unitroot_kpss(cons$DK)
adf.test(cons$DK)
# Perform differencing
cons_diff_dk <- cons %>% mutate(DK = difference(DK,7)) %>% dplyr::filter(!is.na(DK)) #Take first order difference
unitroot_kpss(cons_diff_dk$DK)
adf.test(cons_diff_dk$DK) #Stationary
# New ACF and PACF plots
forecast::ggtsdisplay(cons_diff_dk$DK, plot_type='partial',
lag.max = 24,
theme = theme_bw(),
main = "Elecitricity consumption in Denmark (difference) ACF and PACF plots ")
fit_arima_optimal_cons_dk <-
cons %>%
as_tsibble(index = date) %>%
model(arima_optimal =  ARIMA(DK, stepwise = FALSE, approximation = FALSE))
fit_arima_manual_cons_dk <- cons %>% as_tsibble(index = date) %>%
model(arima_101111       = ARIMA(DK ~ 0 + pdq(1,0,1) + PDQ(1,1,1)))
fit_cons_dk  <-  fit_arima_manual_cons_dk  %>% bind_cols(fit_arima_optimal_cons_dk)
elspot_data <- read_csv("elspot-prices_2019_daily_nok.csv") # Load data
colnames(elspot_data) <- elspot_data[2,]
elspot_data <- elspot_data[3:nrow(elspot_data),]
colnames(elspot_data)[1] <- "date"
elspot_data  %<>%
mutate(date = lubridate::dmy(date),
FI = as.numeric(gsub(",", ".", FI)))  %>%  #Substitute commas
select(date, FI)  %>%
as_tsibble(index = date)
#Plot of finnish electricity prices and ACF/PACF plots
forecast::ggtsdisplay(elspot_data$FI, plot_type='partial',
lag.max = 24,
theme = theme_bw(),
main = "Finnish electricity prices in 2019 NOK mwh")
fi_garch <- garchFit(~arma(1,1) + garch(1,1), data = elspot_data$FI, trace = F)
summary(fi_garch)
fi_garch@residuals
hist(fi_garch@residuals)
ggtsdisplay(fi_garch@residuals)
ggtsdisplay(Residuals,
plot.type = "histogram",
lag.max = 24,
theme = theme_bw(),
main = "Residuals of ETS(A,Ad,A) model")
ggtsdisplay(fi_garch@residuals)
hist(fi_garch@residuals)
ggtsdisplay(fi_garch@residuals)
ggtsdisplay(fi_garch@residuals, plot.type = "histogram")
el_spot <- read.csv("G:\Dokumenter\Google drive folder\NHH\Master\BAN403\Exam\Repository\kolonial data og informasjon\el_spot_prices.csv")
library(tidverse)
el_spot <- read.csv("G:\Dokumenter\Google drive folder\NHH\Master\BAN403\Exam\Repository\kolonial data og informasjon\el_spot_prices.csv")
# Save pv_df
#save(pv_df, file = "pv_df.Rdata")
load("pv_df.Rdata")
texas_gas_gerneration <- getEIA(ID ="EBA.TEX-ALL.NG.NG.HL" , key = key)
setwd("G:/Dokumenter/Google drive folder/NHH/Master/ENE434/Term assignment/repo")
library(EIAdata)
library(fpp3)
library(tidyverse)
library(magrittr)
library(lubridate)
#setwd("G:/Dokumenter/Google drive folder/NHH/Master/ENE434/Term assignment/repo")
key <- "81a7388709d31bb149eb1cc9c7eba736"
## All in local time
texas_nuclear_generation <- getEIA(ID = "EBA.TEX-ALL.NG.NUC.HL", key = key)
texas_gas_gerneration <- getEIA(ID ="EBA.TEX-ALL.NG.NG.HL" , key = key)
texas_generation <- getEIA(ID = "EBA.TEX-ALL.NG.HL", key = key)
texas_demand <- getEIA(ID = "EBA.TEX-ALL.D.HL", key = key)
View(texas_gas_gerneration)
texas_gas_generation <-texas_gas_gerneration
texas_demand %<>% as.data.frame(texas_demand)
dates <- rownames(texas_demand)
texas_demand["date"] <- rownames(lubridate::texas_demand)
rownames(texas_demand) <- seq(nrow(texas_demand))
View(texas_demand)
texas_demand %>% plot()
View(texas_nuclear_generation)
texas_nuclear_generation <- getEIA(ID = "EBA.TEX-ALL.NG.NUC.HL", key = key)
texas_nuclear_generation %<>% as.data.frame(texas_nuclear_generation)
View(texas_nuclear_generation)
texas_nuclear_generation <- getEIA(ID = "EBA.TEX-ALL.NG.NUC.HL", key = key)
texas_nuclear_generation <- getEIA(ID = "EBA.TEX-ALL.NG.NUC.HL", key = key)
rownames(texas_nuclear_generation)
texas_nuclear_generation
texas_nuclear_generation["date"] <- rownames(texas_nuclear_generation)
View(texas_nuclear_generation)
texas_nuclear_generation <- getEIA(ID = "EBA.TEX-ALL.NG.NUC.HL", key = key)
rownames(texas_nuclear_generation)
rownames(texas_nuclear_generation)
View(texas_nuclear_generation)
rownames(texas_nuclear_generation)
rownames(texas_nuclear_generation)
texas_nuclear_generation %>%  mutate(date = rownames(texas_nuclear_generation)
)
texas_nuclear_generation["date"] <- rownames(texas_nuclear_generation)
View(texas_nuclear_generation)
texas_nuclear_generation %>% as.data.frame()
texas_nuclear_generation %>%
as.data.frame() %>%
mutate(date = rownames(texas_nuclear_generation)
)
tibble::view(
texas_nuclear_generation %>%
as.data.frame() %>%
mutate(date = rownames(texas_nuclear_generation))
)
test <-
texas_nuclear_generation %>%
as.data.frame() %>%
mutate(date = rownames(texas_nuclear_generation)
)
test$date
test$date
test
texas_nuclear_generation %>%
as.data.frame() %>%
mutate(date = rownames(texas_nuclear_generation))
test <- texas_nuclear_generation %>%
as.data.frame() %>%
mutate(date = rownames(texas_nuclear_generation))
test
test$EBA.TEX.ALL.NG.NUC.HL
texas_nuclear_generation["date"] <- rownames(texas_nuclear_generation)
View(texas_nuclear_generation)
test["date"] <- rownames(texas_nuclear_generation)
View(test)
typeof(test)
test %>% as.data.frame()
test %>% as.data.frame() %>% mutate(date = rownames(test))
test <- test %>% mutate(date = rownames(test))
test$date
test$date[1] %>% strsplit(.,)
test$date[1]
test$date[1] %>% strsplit(., ".")
test$date[1] %>% strsplit(., )
help(strsplit)
help(strsplit)
test$date[1] %>% strsplit(., split = ".")
test$date[1] %>% strsplit(., split = " ")
test$date[1] %>% strsplit(., split = "", fixed = TRUE)
test$date[1] %>% strsplit(., split = "", fixed = FALSE)
test$date[1] %>% strsplit(., split = "", fixed = FALSE)[[1]]
list <-test$date[1] %>% strsplit(., split = "", fixed = FALSE)
list[[1]]
list[2]
list[[1]][]2
list[[1]][2]
test$date[1]
length <- length(strsplit(test$date[1], split = ".", fixed = FALSE))
length
strsplit(test$date[1], split = ".", fixed = FALSE)
strsplit(test$date[1], split = "", fixed = FALSE)
length <- length(strsplit(test$date[1], split = "", fixed = FALSE))
length
strsplit(test$date[1], split = "", fixed = FALSE)
length(strsplit(test$date[1], split = "", fixed = FALSE)[[1]])
length <-length(strsplit(test$date[1], split = "", fixed = FALSE)[[1]])
length
length <- length(strsplit(test$date[1], split = "", fixed = FALSE)[[1]])
for (i in 2:nrow(test)) {
length_temp = length(strsplit(test$date[1], split = "", fixed = FALSE)[[1]])
if (length != length_temp) print("FALSE")
}
for (i in 2:nrow(test)) {
length_temp = length(strsplit(test$date[1], split = "", fixed = FALSE)[[1]])
if (length != length_temp) print("TRUE")
}
length <- length(strsplit(test$date[1], split = "", fixed = FALSE)[[1]])
for (i in 2:nrow(test)) {
length_temp = length(strsplit(test$date[1], split = "", fixed = FALSE)[[1]])
if (length == length_temp) print("TRUE")
}
test$date %>% strsplit(., split = "", fixed = FALSE)
(test$date %>% strsplit(., split = "", fixed = FALSE))
(test$date %>% strsplit(., split = "", fixed = FALSE))[[1]]
(test$date %>% strsplit(., split = "", fixed = FALSE))[[1]][1:2]
(test$date %>% strsplit(., split = "", fixed = FALSE))[[1]][19:20]
texas_nuclear_generation %>%
as.data.frame() %>%
mutate(date = rownames(texas_nuclear_generation),
hour = strsplit(., split = ""))
texas_nuclear_generation %>%
as.data.frame() %>%
mutate(date = rownames(texas_nuclear_generation),
hour = sapply(strsplit(date, split = "")))
texas_nuclear_generation %>%
as.data.frame() %>%
mutate(date = rownames(texas_nuclear_generation))
texas_nuclear_generation %>%
as.data.frame() %>%
mutate(date = rownames(texas_nuclear_generation)) %>%
mutate(hour = sapply(strsplit(date, split = "")))
texas_nuclear_generation %>%
as.data.frame() %>%
mutate(date = rownames(texas_nuclear_generation),
hour = sapply(strsplit(date, ""), `[`, 2)
)
test %>%  sapply(strsplit(date(''), " "), `[`, 2)
test %>%  sapply(strsplit(date, ""), `[`, 2)
test %>%  sapply(strsplit(date, ""), `[`, 2)
test$date
test %>%  mutate(hour = sapply(strsplit(date, ""), `[`, 2))
test %>%  mutate(hour = sapply(strsplit(date, ""), `[`, 1))
test %>%  mutate(hour = sapply(strsplit(date, ""), `[`, 19:20))
test %>%  mutate(hour = sapply(strsplit(date, ""), `[`, 19))
test %>%  mutate(hour = sapply(strsplit(date, ""), 19))
test %>%  mutate(hour = sapply(strsplit(date, "")))
test %>%  mutate(hour = sapply(strsplit(date, "")))
help(saply)
help(sapply)
test %>%  mutate(hour = sapply(strsplit(date, "")))
test %>%  mutate(hour = sapply(strsplit(date, ""), `[, 19:20))
)
)
ef
,
,,
[
few
wqdq''
,
,
,
data_uk_2015 <- read_xls("../datasett/UK/UK_2015.xls",
sheet = "Weekly Figures 2015",
range = "A3:BB40") %>%
clean_data(.,
wqdq'''
